{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd625a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0458c492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Ensure we're working from project root\n",
    "current_dir = os.getcwd()\n",
    "if current_dir.endswith('notebooks'):\n",
    "    os.chdir('..')\n",
    "    print(\"Adjusted working directory to project root\")\n",
    "\n",
    "import utils.model_monitoring as mm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f9e8d4",
   "metadata": {},
   "source": [
    "## Setup PySpark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f709de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized for model monitoring\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"Model_Monitoring_Pipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark session initialized for model monitoring\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472bf5f",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c12bf769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Monitoring Configuration:\n",
      "{'alert_thresholds': {'auc_critical': 0.1,\n",
      "                      'auc_warning': 0.05,\n",
      "                      'psi_critical': 0.25,\n",
      "                      'psi_warning': 0.1},\n",
      " 'alerts_output_path': 'datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/alerts',\n",
      " 'labels_path': 'datamart/gold/label_store/',\n",
      " 'model_artifact_filepath': 'models/credit_model_xgboost_2024_09_01.pkl',\n",
      " 'model_bank_directory': 'models/',\n",
      " 'model_version': 'credit_model_xgboost_2024_09_01',\n",
      " 'monitoring_end_date': '2024-12-01',\n",
      " 'monitoring_end_dt': datetime.datetime(2024, 12, 1, 0, 0),\n",
      " 'monitoring_output_path': 'datamart/gold/model_monitoring/',\n",
      " 'monitoring_start_date': '2024-10-01',\n",
      " 'monitoring_start_dt': datetime.datetime(2024, 10, 1, 0, 0),\n",
      " 'performance_degradation_threshold': 0.05,\n",
      " 'performance_output_path': 'datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/performance_metrics',\n",
      " 'predictions_path': 'datamart/gold/model_predictions/',\n",
      " 'reports_output_path': 'datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/reports',\n",
      " 'stability_output_path': 'datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/stability_analysis',\n",
      " 'stability_threshold': 0.2,\n",
      " 'visualizations_output_path': 'datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/visualizations'}\n"
     ]
    }
   ],
   "source": [
    "model_version = \"credit_model_xgboost_2024_09_01\"  # Model to monitor\n",
    "monitoring_start_date = \"2024-10-01\"  # Start date for monitoring period\n",
    "monitoring_end_date = \"2024-12-01\"    # End date for monitoring period\n",
    "\n",
    "config = mm.build_monitoring_config(\n",
    "    model_version=model_version,\n",
    "    monitoring_start_date=monitoring_start_date,\n",
    "    monitoring_end_date=monitoring_end_date,\n",
    "    predictions_path=\"datamart/gold/model_predictions/\",\n",
    "    labels_path=\"datamart/gold/label_store/\",\n",
    "    monitoring_output_path=\"datamart/gold/model_monitoring/\",\n",
    "    model_bank_directory=\"models/\"\n",
    ")\n",
    "\n",
    "print(\"Model Monitoring Configuration:\")\n",
    "pprint.pprint(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef772cbd",
   "metadata": {},
   "source": [
    "## Load Model Training Performance Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f7e47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING BASELINE PERFORMANCE\n",
      "============================================================\n",
      "Training baseline loaded from: models/credit_model_xgboost_2024_09_01.pkl\n",
      "Baseline performance loaded successfully\n",
      "Training AUC: 0.9686\n",
      "Test AUC: 0.8927\n",
      "OOT AUC: 0.8845\n",
      "Training date: 2024-09-01\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOADING BASELINE PERFORMANCE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "baseline_performance = mm.load_training_baseline(config)\n",
    "\n",
    "if baseline_performance:\n",
    "    print(f\"Baseline performance loaded successfully\")\n",
    "    print(f\"Training AUC: {baseline_performance['auc_train']:.4f}\")\n",
    "    print(f\"Test AUC: {baseline_performance['auc_test']:.4f}\")\n",
    "    print(f\"OOT AUC: {baseline_performance['auc_oot']:.4f}\")\n",
    "    print(f\"Training date: {baseline_performance.get('training_date', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"Warning: Could not load baseline performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552e46c",
   "metadata": {},
   "source": [
    "## Load Predictions and Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c40a9fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING MONITORING DATA\n",
      "============================================================\n",
      "Loading monitoring data for credit_model_xgboost_2024_09_01\n",
      "Date range: 2024-10-01 to 2024-12-01\n",
      "Loaded 34404 prediction records\n",
      "Loaded 8974 label records (all periods)\n",
      "Merged monitoring data: 26922 records\n",
      "Date coverage: 3 unique dates\n",
      "Monitoring data loaded successfully\n",
      "Total records: 26922\n",
      "Date range: 2024-10-01 to 2024-12-01\n",
      "Unique dates: 3\n",
      "Average records per date: 8974\n",
      "Missing actual labels: 0 (0.0%)\n",
      "Missing predictions: 0 (0.0%)\n",
      "\n",
      "Sample monitoring data:\n",
      "  Customer_ID snapshot_date  model_prediction_proba  actual_label\n",
      "0  CUS_0x4e42    2024-10-01                0.043231             0\n",
      "1  CUS_0x4e47    2024-10-01                0.493364             0\n",
      "2  CUS_0x4e55    2024-10-01                0.830098             0\n",
      "3  CUS_0x4e60    2024-10-01                0.840814             1\n",
      "4  CUS_0x4e7d    2024-10-01                0.542185             0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOADING MONITORING DATA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "monitoring_data = mm.load_monitoring_data(config, spark)\n",
    "\n",
    "if monitoring_data is not None and len(monitoring_data) > 0:\n",
    "    print(f\"Monitoring data loaded successfully\")\n",
    "    print(f\"Total records: {len(monitoring_data)}\")\n",
    "    print(f\"Date range: {monitoring_data['snapshot_date'].min()} to {monitoring_data['snapshot_date'].max()}\")\n",
    "    print(f\"Unique dates: {monitoring_data['snapshot_date'].nunique()}\")\n",
    "    print(f\"Average records per date: {len(monitoring_data) / monitoring_data['snapshot_date'].nunique():.0f}\")\n",
    "    \n",
    "    missing_labels = monitoring_data['actual_label'].isna().sum()\n",
    "    missing_predictions = monitoring_data['model_prediction_proba'].isna().sum()\n",
    "    print(f\"Missing actual labels: {missing_labels} ({missing_labels/len(monitoring_data)*100:.1f}%)\")\n",
    "    print(f\"Missing predictions: {missing_predictions} ({missing_predictions/len(monitoring_data)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSample monitoring data:\")\n",
    "    print(monitoring_data[['Customer_ID', 'snapshot_date', 'model_prediction_proba', 'actual_label']].head())\n",
    "else:\n",
    "    raise ValueError(\"Failed to load monitoring data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61803d67",
   "metadata": {},
   "source": [
    "## Calculate Performance Metrics Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68aa950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CALCULATING PERFORMANCE METRICS\n",
      "============================================================\n",
      "Calculating performance metrics over time...\n",
      "Performance metrics calculated for 3 time periods\n",
      "Performance metrics calculated successfully\n",
      "Monitoring periods: 3\n",
      "\n",
      "Performance Summary:\n",
      "AUC Range: [0.8881, 0.8912]\n",
      "Mean AUC: 0.8895 Â± 0.0015\n",
      "Gini Range: [0.776, 0.782]\n",
      "Mean Gini: 0.779 Â± 0.003\n",
      "\n",
      "Baseline Comparison:\n",
      "Baseline AUC: 0.8845\n",
      "Current Mean AUC: 0.8895\n",
      "Performance Change: +0.0050 (+0.6%)\n",
      "\n",
      "Detailed Performance Metrics:\n",
      "  snapshot_date  sample_count  actual_default_rate  predicted_default_rate  \\\n",
      "0    2024-10-01          8974             0.288723                0.302764   \n",
      "1    2024-11-01          8974             0.288723                0.297526   \n",
      "2    2024-12-01          8974             0.288723                0.297415   \n",
      "\n",
      "        auc      gini  precision    recall  f1_score  mean_prediction  \\\n",
      "0  0.888137  0.776274   0.695988  0.729834  0.712509         0.340999   \n",
      "1  0.889167  0.778334   0.702622  0.724045  0.713172         0.339934   \n",
      "2  0.891160  0.782321   0.702510  0.723659  0.712928         0.340832   \n",
      "\n",
      "   std_prediction  true_positives  false_positives  true_negatives  \\\n",
      "0        0.311501            1891              826            5557   \n",
      "1        0.309416            1876              794            5589   \n",
      "2        0.309805            1875              794            5589   \n",
      "\n",
      "   false_negatives  \n",
      "0              700  \n",
      "1              715  \n",
      "2              716  \n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CALCULATING PERFORMANCE METRICS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "performance_metrics = mm.calculate_performance_metrics_over_time(\n",
    "    monitoring_data=monitoring_data,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if performance_metrics is not None and len(performance_metrics) > 0:\n",
    "    print(f\"Performance metrics calculated successfully\")\n",
    "    print(f\"Monitoring periods: {len(performance_metrics)}\")\n",
    "    \n",
    "    print(f\"\\nPerformance Summary:\")\n",
    "    print(f\"AUC Range: [{performance_metrics['auc'].min():.4f}, {performance_metrics['auc'].max():.4f}]\")\n",
    "    print(f\"Mean AUC: {performance_metrics['auc'].mean():.4f} Â± {performance_metrics['auc'].std():.4f}\")\n",
    "    print(f\"Gini Range: [{performance_metrics['gini'].min():.3f}, {performance_metrics['gini'].max():.3f}]\")\n",
    "    print(f\"Mean Gini: {performance_metrics['gini'].mean():.3f} Â± {performance_metrics['gini'].std():.3f}\")\n",
    "    \n",
    "    if baseline_performance:\n",
    "        baseline_auc = baseline_performance.get('auc_oot', baseline_performance.get('auc_test', 0))\n",
    "        current_mean_auc = performance_metrics['auc'].mean()\n",
    "        auc_difference = current_mean_auc - baseline_auc\n",
    "        print(f\"\\nBaseline Comparison:\")\n",
    "        print(f\"Baseline AUC: {baseline_auc:.4f}\")\n",
    "        print(f\"Current Mean AUC: {current_mean_auc:.4f}\")\n",
    "        print(f\"Performance Change: {auc_difference:+.4f} ({auc_difference/baseline_auc*100:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nDetailed Performance Metrics:\")\n",
    "    print(performance_metrics)\n",
    "else:\n",
    "    raise ValueError(\"Failed to calculate performance metrics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b0985",
   "metadata": {},
   "source": [
    "## Analyze Data Stability and Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73205996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYZING DATA STABILITY\n",
      "============================================================\n",
      "Analyzing data stability and drift...\n",
      "Stability analysis completed\n",
      "Stability analysis completed successfully\n",
      "\n",
      "Prediction Distribution Analysis:\n",
      "Mean prediction drift: 0.0006\n",
      "Std prediction drift: 0.0011\n",
      "Distribution shift (KS statistic): 0.0097\n",
      "âœ“ Prediction distribution is stable\n",
      "\n",
      "Feature Drift Analysis:\n",
      "High drift features (PSI > 0.2): 1\n",
      "Moderate drift features (PSI 0.1-0.2): 0\n",
      "High drift features: ['prediction_stability_score']\n",
      "\n",
      "Model Stability Metrics:\n",
      "Performance volatility: 0.0015\n",
      "Consistency score: 0.9983\n",
      "âœ“ Model performance is stable\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ANALYZING DATA STABILITY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "stability_analysis = mm.analyze_data_stability(\n",
    "    monitoring_data=monitoring_data,\n",
    "    config=config,\n",
    "    baseline_performance=baseline_performance\n",
    ")\n",
    "\n",
    "if stability_analysis is not None:\n",
    "    print(f\"Stability analysis completed successfully\")\n",
    "    \n",
    "    pred_stability = stability_analysis.get('prediction_stability', {})\n",
    "    if pred_stability:\n",
    "        print(f\"\\nPrediction Distribution Analysis:\")\n",
    "        print(f\"Mean prediction drift: {pred_stability.get('mean_drift', 0):.4f}\")\n",
    "        print(f\"Std prediction drift: {pred_stability.get('std_drift', 0):.4f}\")\n",
    "        print(f\"Distribution shift (KS statistic): {pred_stability.get('ks_statistic', 0):.4f}\")\n",
    "        \n",
    "        if pred_stability.get('is_stable', True):\n",
    "            print(f\"âœ“ Prediction distribution is stable\")\n",
    "        else:\n",
    "            print(f\"âš  Warning: Prediction distribution shows significant drift\")\n",
    "    \n",
    "    feature_drift = stability_analysis.get('feature_drift', {})\n",
    "    if feature_drift:\n",
    "        print(f\"\\nFeature Drift Analysis:\")\n",
    "        high_drift_features = [f for f, psi in feature_drift.items() if psi > 0.2]\n",
    "        moderate_drift_features = [f for f, psi in feature_drift.items() if 0.1 < psi <= 0.2]\n",
    "        \n",
    "        print(f\"High drift features (PSI > 0.2): {len(high_drift_features)}\")\n",
    "        print(f\"Moderate drift features (PSI 0.1-0.2): {len(moderate_drift_features)}\")\n",
    "        \n",
    "        if high_drift_features:\n",
    "            print(f\"High drift features: {high_drift_features[:5]}\")  # Show first 5\n",
    "    \n",
    "    model_stability = stability_analysis.get('model_stability', {})\n",
    "    if model_stability:\n",
    "        print(f\"\\nModel Stability Metrics:\")\n",
    "        print(f\"Performance volatility: {model_stability.get('performance_volatility', 0):.4f}\")\n",
    "        print(f\"Consistency score: {model_stability.get('consistency_score', 0):.4f}\")\n",
    "        \n",
    "        if model_stability.get('is_stable', True):\n",
    "            print(f\"âœ“ Model performance is stable\")\n",
    "        else:\n",
    "            print(f\"âš  Warning: Model performance shows instability\")\n",
    "else:\n",
    "    print(\"Warning: Stability analysis failed or returned no results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92d68b",
   "metadata": {},
   "source": [
    "## Generate Performance Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f05243c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING MONITORING VISUALIZATIONS\n",
      "============================================================\n",
      "Generated 3 monitoring charts\n",
      "Visualizations generated successfully\n",
      "Charts created: 3\n",
      "  performance_trend: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/visualizations/performance_trend.png\n",
      "  prediction_distribution: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/visualizations/prediction_distribution.png\n",
      "  performance_vs_volume: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/visualizations/performance_vs_volume.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance trend chart displayed above\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GENERATING MONITORING VISUALIZATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "visualization_paths = mm.generate_monitoring_visualizations(\n",
    "    performance_metrics=performance_metrics,\n",
    "    stability_analysis=stability_analysis,\n",
    "    monitoring_data=monitoring_data,\n",
    "    baseline_performance=baseline_performance,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if visualization_paths:\n",
    "    print(f\"Visualizations generated successfully\")\n",
    "    print(f\"Charts created: {len(visualization_paths)}\")\n",
    "    \n",
    "    for chart_name, file_path in visualization_paths.items():\n",
    "        print(f\"  {chart_name}: {file_path}\")\n",
    "    \n",
    "    if 'performance_trend' in visualization_paths:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.show()\n",
    "        print(f\"Performance trend chart displayed above\")\n",
    "else:\n",
    "    print(\"Warning: Failed to generate visualizations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb3df88",
   "metadata": {},
   "source": [
    "## Performance Alert Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a6564e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE ALERT DETECTION\n",
      "============================================================\n",
      "Generated 0 monitoring alerts\n",
      "No alerts detected - model performance is stable\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE ALERT DETECTION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "alerts = mm.detect_performance_alerts(\n",
    "    performance_metrics=performance_metrics,\n",
    "    stability_analysis=stability_analysis,\n",
    "    baseline_performance=baseline_performance,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if alerts:\n",
    "    print(f\"Alert detection completed\")\n",
    "    print(f\"Total alerts: {len(alerts)}\")\n",
    "    \n",
    "    critical_alerts = [a for a in alerts if a.get('severity') == 'critical']\n",
    "    warning_alerts = [a for a in alerts if a.get('severity') == 'warning']\n",
    "    info_alerts = [a for a in alerts if a.get('severity') == 'info']\n",
    "    \n",
    "    print(f\"Critical alerts: {len(critical_alerts)}\")\n",
    "    print(f\"Warning alerts: {len(warning_alerts)}\")\n",
    "    print(f\"Info alerts: {len(info_alerts)}\")\n",
    "    \n",
    "    if critical_alerts:\n",
    "        print(f\"\\nðŸš¨ CRITICAL ALERTS:\")\n",
    "        for alert in critical_alerts[:3]:  # Show first 3\n",
    "            print(f\"  - {alert.get('message', 'Unknown alert')}\")\n",
    "            print(f\"    Date: {alert.get('date', 'Unknown')}\")\n",
    "            print(f\"    Metric: {alert.get('metric', 'Unknown')}\")\n",
    "            print(f\"    Value: {alert.get('value', 'Unknown')}\")\n",
    "    \n",
    "    if warning_alerts:\n",
    "        print(f\"\\nâš ï¸ WARNING ALERTS:\")\n",
    "        for alert in warning_alerts[:3]:  # Show first 3\n",
    "            print(f\"  - {alert.get('message', 'Unknown alert')}\")\n",
    "    \n",
    "    if not critical_alerts and not warning_alerts:\n",
    "        print(f\"âœ“ No critical or warning alerts detected\")\n",
    "        print(f\"Model performance appears stable\")\n",
    "else:\n",
    "    print(\"No alerts detected - model performance is stable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6a561e",
   "metadata": {},
   "source": [
    "## Save Monitoring Results to Datamart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "569543ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MONITORING RESULTS\n",
      "============================================================\n",
      "Monitoring results saved successfully\n",
      "Monitoring results saved successfully\n",
      "Performance metrics: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/performance_metrics/performance_metrics_2024-10-01_2024-12-01_2025_06_28_08_54_14.parquet\n",
      "Stability analysis: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/stability_analysis/stability_analysis_2024-10-01_2024-12-01_2025_06_28_08_54_14.pkl\n",
      "Alerts: No alerts generated\n",
      "Monitoring summary saved to: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/reports/monitoring_summary_2024-10-01_2024-12-01_2025_06_28_08_54_14.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SAVING MONITORING RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "performance_output_path = mm.save_monitoring_results(\n",
    "    performance_metrics=performance_metrics,\n",
    "    stability_analysis=stability_analysis,\n",
    "    alerts=alerts,\n",
    "    config=config,\n",
    "    spark=spark\n",
    ")\n",
    "\n",
    "if performance_output_path:\n",
    "    print(f\"Monitoring results saved successfully\")\n",
    "    print(f\"Performance metrics: {performance_output_path['performance_metrics']}\")\n",
    "    print(f\"Stability analysis: {performance_output_path['stability_analysis']}\")\n",
    "\n",
    "    alerts_path = performance_output_path.get('alerts')\n",
    "    print(f\"Alerts: {alerts_path if alerts_path else 'No alerts generated'}\")\n",
    "    \n",
    "    summary_output_path = mm.save_monitoring_summary(\n",
    "        performance_metrics=performance_metrics,\n",
    "        stability_analysis=stability_analysis,\n",
    "        alerts=alerts,\n",
    "        baseline_performance=baseline_performance,\n",
    "        config=config\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Failed to save monitoring results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8fc15",
   "metadata": {},
   "source": [
    "## Batch Monitoring for Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c612c9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BATCH MONITORING SETUP\n",
      "================================================================================\n",
      "Batch monitoring configuration:\n",
      "Models to monitor: 3\n",
      "Time period: 2024-04-01 to 2024-06-01\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BATCH MONITORING SETUP\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "batch_models = [\n",
    "    \"credit_model_logistic_regression_2024_09_01\",\n",
    "    \"credit_model_random_forest_2024_09_01\",\n",
    "    \"credit_model_xgboost_2024_09_01\"\n",
    "]\n",
    "\n",
    "batch_monitoring_config = {\n",
    "    \"models\": batch_models,\n",
    "    \"monitoring_start_date\": \"2024-04-01\",\n",
    "    \"monitoring_end_date\": \"2024-06-01\",\n",
    "    \"monitoring_frequency\": \"monthly\"  # weekly, monthly, daily\n",
    "}\n",
    "\n",
    "print(f\"Batch monitoring configuration:\")\n",
    "print(f\"Models to monitor: {len(batch_models)}\")\n",
    "print(f\"Time period: {batch_monitoring_config['monitoring_start_date']} to {batch_monitoring_config['monitoring_end_date']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db0434",
   "metadata": {},
   "source": [
    "Execute batch monitoring\n",
    "Summary of batch monitoring results\n",
    "Summary statistics across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0025662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXECUTING BATCH MONITORING\n",
      "============================================================\n",
      "Starting batch monitoring for 3 models\n",
      "Monitoring period: 2024-04-01 to 2024-06-01\n",
      "\n",
      "Processing model 1/3: credit_model_logistic_regression_2024_09_01\n",
      "Training baseline loaded from: models/credit_model_logistic_regression_2024_09_01.pkl\n",
      "Loading monitoring data for credit_model_logistic_regression_2024_09_01\n",
      "Date range: 2024-04-01 to 2024-06-01\n",
      "Loaded 26922 prediction records\n",
      "Loaded 8974 label records (all periods)\n",
      "Merged monitoring data: 26922 records\n",
      "Date coverage: 3 unique dates\n",
      "Calculating performance metrics over time...\n",
      "Performance metrics calculated for 3 time periods\n",
      "Analyzing data stability and drift...\n",
      "Stability analysis completed\n",
      "Generated 0 monitoring alerts\n",
      "Monitoring results saved successfully\n",
      "âœ“ Successfully monitored credit_model_logistic_regression_2024_09_01\n",
      "\n",
      "Processing model 2/3: credit_model_random_forest_2024_09_01\n",
      "Training baseline loaded from: models/credit_model_random_forest_2024_09_01.pkl\n",
      "Loading monitoring data for credit_model_random_forest_2024_09_01\n",
      "Date range: 2024-04-01 to 2024-06-01\n",
      "Loaded 26922 prediction records\n",
      "Loaded 8974 label records (all periods)\n",
      "Merged monitoring data: 26922 records\n",
      "Date coverage: 3 unique dates\n",
      "Calculating performance metrics over time...\n",
      "Performance metrics calculated for 3 time periods\n",
      "Analyzing data stability and drift...\n",
      "Stability analysis completed\n",
      "Generated 0 monitoring alerts\n",
      "Monitoring results saved successfully\n",
      "âœ“ Successfully monitored credit_model_random_forest_2024_09_01\n",
      "\n",
      "Processing model 3/3: credit_model_xgboost_2024_09_01\n",
      "Training baseline loaded from: models/credit_model_xgboost_2024_09_01.pkl\n",
      "Loading monitoring data for credit_model_xgboost_2024_09_01\n",
      "Date range: 2024-04-01 to 2024-06-01\n",
      "Loaded 26922 prediction records\n",
      "Loaded 8974 label records (all periods)\n",
      "Merged monitoring data: 26922 records\n",
      "Date coverage: 3 unique dates\n",
      "Calculating performance metrics over time...\n",
      "Performance metrics calculated for 3 time periods\n",
      "Analyzing data stability and drift...\n",
      "Stability analysis completed\n",
      "Generated 0 monitoring alerts\n",
      "Monitoring results saved successfully\n",
      "âœ“ Successfully monitored credit_model_xgboost_2024_09_01\n",
      "\n",
      "Batch monitoring completed:\n",
      "Successful: 3 models\n",
      "Failed: 0 models\n",
      "Successfully monitored: credit_model_logistic_regression_2024_09_01, credit_model_random_forest_2024_09_01, credit_model_xgboost_2024_09_01\n",
      "\n",
      "Batch Summary:\n",
      "Total alerts across all models: 0\n",
      "Average performance (AUC): 0.8520\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXECUTING BATCH MONITORING\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "batch_results = mm.run_batch_monitoring(\n",
    "    models_list=batch_models,\n",
    "    monitoring_start_date=batch_monitoring_config[\"monitoring_start_date\"],\n",
    "    monitoring_end_date=batch_monitoring_config[\"monitoring_end_date\"],\n",
    "    spark=spark,\n",
    "    predictions_path=\"datamart/gold/model_predictions/\",\n",
    "    labels_path=\"datamart/gold/label_store/\",\n",
    "    monitoring_output_path=\"datamart/gold/model_monitoring/\",\n",
    "    model_bank_directory=\"models/\"\n",
    ")\n",
    "\n",
    "if batch_results:\n",
    "    successful_models = [result['model'] for result in batch_results if result['success']]\n",
    "    failed_models = [result['model'] for result in batch_results if not result['success']]\n",
    "    \n",
    "    print(f\"\\nBatch monitoring completed:\")\n",
    "    print(f\"Successful: {len(successful_models)} models\")\n",
    "    print(f\"Failed: {len(failed_models)} models\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(f\"Successfully monitored: {', '.join(successful_models)}\")\n",
    "    if failed_models:\n",
    "        print(f\"Failed to monitor: {', '.join(failed_models)}\")\n",
    "    \n",
    "    total_alerts = sum([len(result.get('alerts', [])) for result in batch_results if result['success']])\n",
    "    avg_performance = np.mean([result.get('avg_auc', 0) for result in batch_results if result['success']])\n",
    "    \n",
    "    print(f\"\\nBatch Summary:\")\n",
    "    print(f\"Total alerts across all models: {total_alerts}\")\n",
    "    print(f\"Average performance (AUC): {avg_performance:.4f}\")\n",
    "else:\n",
    "    print(\"Batch monitoring failed to complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0170b860",
   "metadata": {},
   "source": [
    "## Generate Monitoring Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88043eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING MONITORING REPORT\n",
      "================================================================================\n",
      "Monitoring report generated: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/reports/monitoring_report_credit_model_xgboost_2024_09_01_2025_06_28_08_54_43.md\n",
      "Comprehensive monitoring report generated\n",
      "Report location: datamart/gold/model_monitoring/credit_model_xgboost_2024_09_01/reports/monitoring_report_credit_model_xgboost_2024_09_01_2025_06_28_08_54_43.md\n",
      "Report includes:\n",
      "  - Executive summary\n",
      "  - Performance trends analysis\n",
      "  - Stability assessment\n",
      "  - Alert summary\n",
      "  - Recommendations\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"GENERATING MONITORING REPORT\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "report_path = mm.generate_monitoring_report(\n",
    "    model_version=model_version,\n",
    "    performance_metrics=performance_metrics,\n",
    "    stability_analysis=stability_analysis,\n",
    "    alerts=alerts,\n",
    "    baseline_performance=baseline_performance,\n",
    "    visualization_paths=visualization_paths,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "if report_path:\n",
    "    print(f\"Comprehensive monitoring report generated\")\n",
    "    print(f\"Report location: {report_path}\")\n",
    "    print(f\"Report includes:\")\n",
    "    print(f\"  - Executive summary\")\n",
    "    print(f\"  - Performance trends analysis\")\n",
    "    print(f\"  - Stability assessment\")\n",
    "    print(f\"  - Alert summary\")\n",
    "    print(f\"  - Recommendations\")\n",
    "else:\n",
    "    print(\"Warning: Failed to generate monitoring report\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebc199",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0bb61d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MONITORING SESSION COMPLETE\n",
      "================================================================================\n",
      "Model Monitored: credit_model_xgboost_2024_09_01\n",
      "Monitoring Period: 2024-10-01 to 2024-12-01\n",
      "Records Analyzed: 26922\n",
      "Performance Periods: 3\n",
      "\n",
      "âœ… Model Health: HEALTHY - No alerts detected\n",
      "Latest AUC: 0.8912\n",
      "Performance vs Baseline: +0.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MONITORING SESSION COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"Model Monitored: {model_version}\")\n",
    "print(f\"Monitoring Period: {monitoring_start_date} to {monitoring_end_date}\")\n",
    "print(f\"Records Analyzed: {len(monitoring_data) if 'monitoring_data' in locals() else 0}\")\n",
    "print(f\"Performance Periods: {len(performance_metrics) if 'performance_metrics' in locals() else 0}\")\n",
    "\n",
    "if 'alerts' in locals() and alerts:\n",
    "    critical_count = len([a for a in alerts if a.get('severity') == 'critical'])\n",
    "    if critical_count > 0:\n",
    "        print(f\"\\nðŸš¨ Model Health: CRITICAL - {critical_count} critical alerts\")\n",
    "    else:\n",
    "        warning_count = len([a for a in alerts if a.get('severity') == 'warning'])\n",
    "        if warning_count > 0:\n",
    "            print(f\"\\nâš ï¸ Model Health: WARNING - {warning_count} warnings\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… Model Health: HEALTHY - No critical issues detected\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Model Health: HEALTHY - No alerts detected\")\n",
    "\n",
    "if 'performance_metrics' in locals() and len(performance_metrics) > 0:\n",
    "    current_auc = performance_metrics['auc'].iloc[-1]  # Latest performance\n",
    "    print(f\"Latest AUC: {current_auc:.4f}\")\n",
    "    \n",
    "    if baseline_performance and 'auc_oot' in baseline_performance:\n",
    "        baseline_auc = baseline_performance['auc_oot']\n",
    "        change = ((current_auc - baseline_auc) / baseline_auc) * 100\n",
    "        print(f\"Performance vs Baseline: {change:+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18244354",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
