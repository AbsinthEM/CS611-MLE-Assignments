services:
  # Jupyter service for development
  jupyter:
    build: .
    image: assignment_2-ml-pipeline
    container_name: assignment2_jupyter_container
    ports:
      - "8888:8888"
    volumes:
      - .:/opt/airflow
    working_dir: /opt/airflow
    entrypoint: ""
    command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root", "--notebook-dir=/opt/airflow", "--ServerApp.token=''", "--ServerApp.disable_check_xsrf=True"]

  # Airflow initialization service
  airflow-init:
    build: .
    image: assignment_2-ml-pipeline
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DISABLED_PROVIDERS=google
    volumes:
      - airflow_data:/opt/airflow              
      - ./dags:/opt/airflow/dags               
      - ./utils:/opt/airflow/utils             
      - ./data:/opt/airflow/data             
      - ./datamart:/opt/airflow/datamart      
      - ./models:/opt/airflow/models 
    entrypoint: >
      /bin/bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
      "

  # Airflow webserver
  airflow-webserver:
    build: .
    image: assignment_2-ml-pipeline
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DISABLED_PROVIDERS=google
    volumes:
      - airflow_data:/opt/airflow              
      - ./dags:/opt/airflow/dags               
      - ./utils:/opt/airflow/utils             
      - ./data:/opt/airflow/data             
      - ./datamart:/opt/airflow/datamart      
      - ./models:/opt/airflow/models 
    ports:
      - "8080:8080"
    command: webserver

  # Airflow scheduler
  airflow-scheduler:
    build: .
    image: assignment_2-ml-pipeline
    depends_on:
      - airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DISABLED_PROVIDERS=google
    volumes:
      - airflow_data:/opt/airflow              
      - ./dags:/opt/airflow/dags               
      - ./utils:/opt/airflow/utils             
      - ./data:/opt/airflow/data             
      - ./datamart:/opt/airflow/datamart      
      - ./models:/opt/airflow/models 
    command: scheduler

volumes:
  airflow_data: